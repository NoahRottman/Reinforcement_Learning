{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # For Deep learning \n",
    "import numpy as np           \n",
    "import retro                 # Retro Environment\n",
    "from retro import *\n",
    "\n",
    "\n",
    "from skimage import transform #preprocess frames\n",
    "from skimage.color import rgb2gray #gray frames\n",
    "\n",
    "import matplotlib.pyplot as plt #Graphing tool\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings # This ignore all the warning messages that are printed during the training due to skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game='SpaceInvaders-Atari2600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of our frame is: \", env.observation_space) #(pixels x pixels x number of pics)\n",
    "print(\"The action size is : \", env.action_space.n)\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
    "# Let's do this with an identity matrix!!!\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    #The purpose of this function is to reduce the computational power required for training\n",
    "    gray = rgb2gray(frame)                                            #GRAY scale frame\n",
    "    cropped_frame = gray[8:-12,4:-12]                                 #CROP the frame [Up: Down, Left: right]\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110,84]) #RESIZE the frame\n",
    "    \n",
    "    return preprocessed_frame #frame with dimensions 110x84x1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT MY EXPLANATION BELOW\n",
    "\n",
    "#Stacking frames is really important because it helps us to give have a sense of motion to our Neural Network.\n",
    "\n",
    "#BUT, we don't stack each frames, we skip 4 frames at each timestep. This means that only every fourth frame is considered. And then, we use this frame to form the stack_frame.\n",
    "\n",
    "#The frame skipping method is already implemented in the library.\n",
    "\n",
    "   # First we preprocess frame\n",
    "   # Then we append the frame to the deque that automatically removes the oldest frame\n",
    "    #Finally we build the stacked state\n",
    "\n",
    "#This is how work stack:\n",
    "\n",
    "    #For the first frame, we feed 4 frames\n",
    "    #At each timestep, we add the new frame to deque and then we stack them to form a new stacked frame\n",
    "    #And so on stack\n",
    "    #If we're done, we create a new stack with 4 new frames (because we are in a new episode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "\n",
    "#Initialize deque\n",
    "#Zero-images and one array for each image\n",
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, restarting): \n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if restarting: #check if starting new episode \n",
    "        #reset the stacked frames\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        #Stack first frame four times since it's all we have at the beginning of an episode\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        state_stack = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else: #since not restarting we simply add the new frame to the stack which auto-deletes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "        state_stack = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    return state_stack, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "state_size = [110,84,4] #dimension of pics by 4 since four frames in stack\n",
    "action_size = env.action_space.n #from above (cell 3) we know this value to be 8\n",
    "alpha = 0.00025 #learning rate\n",
    "\n",
    "#TRAINING\n",
    "num_episodes = 75\n",
    "max_steps = 50000\n",
    "batch_size = 64\n",
    "\n",
    "#EXPLORATION VS. EXPLOITATION\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay = 0.00001\n",
    "\n",
    "#DISCOUNTING\n",
    "gamma = 0.9\n",
    "\n",
    "#MEMORY\n",
    "pretrain_length = batch_size\n",
    "memory = 1000000\n",
    "\n",
    "#preproc.\n",
    "stack_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN: #Setting up the neural network\n",
    "    def __init__(self, state_size, action_size, alpha, name = \"DQN\"):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # for input state_size is like [none,84,84,4]\n",
    "            self.input = tf.placeholder(tf.float32, [None,*state_size], name = \"input\")\n",
    "            self.action = tf.placeholder(tf.float32, [None,self.action_size], name = \"action\")\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name = \"target\")\n",
    "            \n",
    "            #CNN 1\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size = [8,8], strides = [4,4], padding = \"VALID\", kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name=\"conv1\")\n",
    "            self.conv1_output = tf.nn.elu(self.conv1, name = \"conv1_output\")\n",
    "            \n",
    "            #CNN 2 (looks cleaner with this code struct)\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_output,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "\n",
    "            self.conv2_output = tf.nn.elu(self.conv2, name=\"conv2_output\") \n",
    "            \n",
    "            #CNN 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_output,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [3,3],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "\n",
    "            self.conv3_output = tf.nn.elu(self.conv3, name=\"conv3_output\")\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_output)\n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = self.action_size, \n",
    "                                        activation=None)\n",
    "            \n",
    "            # predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.action))\n",
    "            \n",
    "            #(Qtarget - Q)^2 for loss\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.alpha).minimize(self.loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "DQN = DQN(state_size, action_size, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience): \n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size): #allows us to take a random sample from the memory, ensures learning properly\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepopulate the memory by taking random actions and recording (S,A,R,new S)\n",
    "\n",
    "memory = Memory(max_size = memory)\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    if i == 0:                #check if first step\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True) #TRUE indicates first step\n",
    "    \n",
    "    choice = random.randint(1,len(possible_actions))-1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    \n",
    "    if done: #if episode is done\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state, done)) #adding exp. to memory\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state #our state is now what was our next state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQN.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    # Epsilon Greedy \n",
    "\n",
    "    EvE = np.random.rand() #exploitation vs. exploration\n",
    "\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > EvE):\n",
    "        #Explore with random action\n",
    "        choice = random.randint(1,len(possible_actions))-1\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    else:\n",
    "        # Exploit based on current knowledge from Q-Network (make estimate first)\n",
    "        Qs = sess.run(DQN.output, feed_dict = {DQN.input: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "                \n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_rate = 0 #decay rate will increase as we train (see line 16)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            state = env.reset() #set first state\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step+=1\n",
    "                decay_rate+=1\n",
    "                #choose action using above function\n",
    "                action, explore_probability = select_action(max_epsilon, min_epsilon, \n",
    "                                                             decay, decay_rate, state, possible_actions)\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action) #do action and get results\n",
    "                \n",
    "                if episode_render:\n",
    "                    env.render()\n",
    "                    episode_rewards.append(reward)\n",
    "                    \n",
    "                if done:\n",
    "                    next_state = np.zeros((110,84), dtype = np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, state, False)\n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                                  'Total reward: {}'.format(total_reward),\n",
    "                                  'Explore P: {:.4f}'.format(explore_probability),\n",
    "                                'Training Loss {:.4f}'.format(loss))\n",
    "                    \n",
    "                    rewards_list.append((episode, total_reward))\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                \n",
    "                else:\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                \n",
    "                #Now the training part\n",
    "                \n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                Qs_next_state = sess.run(DQN.output, feed_dict = {DQN.input: next_states_mb})\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    ending = dones_mb[i]\n",
    "                    \n",
    "                    if ending: #if the episode ends at the next state we only get the current reward\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma*(np.max(Qs_next_state[i]))\n",
    "                        target_Qs_batch.append(target)\n",
    "                    \n",
    "                    targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                    \n",
    "                    loss, _ = sess.run([DQN.loss, DQN.optimizer],\n",
    "                                        feed_dict={DQN.input: states_mb,\n",
    "                                                   DQN.target_Q: targets_mb,\n",
    "                                                   DQN.action: actions_mb})\n",
    "\n",
    "                #TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQN.input: states_mb,\n",
    "                                                       DQN.target_Q: targets_mb,\n",
    "                                                       DQN.action: actions_mb})\n",
    "                \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1):\n",
    "        total_rewards = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "        \n",
    "        while True:\n",
    "            # Reshape the state\n",
    "            state = state.reshape((1, *state_size))\n",
    "            # Get action from Q-network \n",
    "            # Estimate the Qs values state\n",
    "            Qs = sess.run(DQN.output, feed_dict = {DQN.input: state})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = select_action[choice]\n",
    "            \n",
    "            #Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print (\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "                \n",
    "                \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
